{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRV_7PdLIXhh"
      },
      "source": [
        "# Prática: Redes Neurais Convolucionais\n",
        "\n",
        "Vamos agora implementar a rede [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), uma das redes que trouxeram todo esse interesse para a área de *deep learning*.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "EAgvY5eBIc1s"
      },
      "outputs": [],
      "source": [
        "import time, os, sys, numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import optim\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "import time, os, sys, numpy as np\n",
        "\n",
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids= list(range(n))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Xru0TyJXIjGp"
      },
      "outputs": [],
      "source": [
        "def load_data_cifar10(batch_size, resize=None, root=os.path.join(\n",
        "        '~', '.pytorch', 'datasets', 'cifar10')):\n",
        "    \"\"\"Download the Cifar10 dataset and then load into memory.\"\"\"\n",
        "    root = os.path.expanduser(root)\n",
        "    transformer = []\n",
        "    if resize:\n",
        "        transformer += [torchvision.transforms.Resize(resize)]\n",
        "    transformer += [torchvision.transforms.ToTensor()]\n",
        "    transformer = torchvision.transforms.Compose(transformer)\n",
        "\n",
        "    mnist_train = torchvision.datasets.CIFAR10(root=root, train=True,download=True,transform=transformer)\n",
        "    mnist_test = torchvision.datasets.CIFAR10(root=root, train=False,download=True,transform=transformer)\n",
        "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(mnist_train,\n",
        "                                  batch_size, shuffle=True,\n",
        "                                  num_workers=num_workers)\n",
        "    test_iter = torch.utils.data.DataLoader(mnist_test,\n",
        "                                 batch_size, shuffle=False,\n",
        "                                 num_workers=num_workers)\n",
        "    return train_iter, test_iter\n",
        "\n",
        "\n",
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    acc_sum, n, l = torch.Tensor([0]), 0, 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y).sum()\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "          n += y.size()[0]\n",
        "\n",
        "    return acc_sum.item() / n, l.item() / len(data_iter)\n",
        "\n",
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n",
        "                   num_epochs):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        net.train()\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_hat = net(X)\n",
        "            trainer.zero_grad()\n",
        "            l = loss(y_hat, y).sum()\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size()[0]\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
        "                 test_acc, time.time() - start))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6YizcSIpiH"
      },
      "source": [
        "## AlexNet\n",
        "\n",
        "Agora já temos todo o conhecimento necessário para implementar nossa primeira arquitetura moderna.\n",
        "Vamos implementar a [AlexNet](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf), uma das arquiteturas mais famosas dessa nova onda de rede neurais.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=700 src=\"https://www.researchgate.net/profile/Jaime_Gallego2/publication/318168077/figure/fig1/AS:578190894927872@1514862859810/AlexNet-CNN-architecture-layers.png\">\n",
        "</p>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=700 src=\"https://miro.medium.com/max/700/1*vXBvV_Unz3JAxytc5iSeoQ.png\">\n",
        "</p>\n",
        "\n",
        "Lembre-se que, após cada camada de convolução e linear, há uma ativação não linear ReLU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "n49zil6YlpwL"
      },
      "outputs": [],
      "source": [
        "root = os.path.join('~', '.pytorch', 'datasets', 'cifar10')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LMWfNHpvIoRR",
        "outputId": "2e6ceb74-1f8d-4556-af1f-8007bfb45626",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 96, 55, 55]          34,944\n",
            "              ReLU-2           [-1, 96, 55, 55]               0\n",
            "         MaxPool2d-3           [-1, 96, 27, 27]               0\n",
            "            Conv2d-4          [-1, 256, 27, 27]         614,656\n",
            "              ReLU-5          [-1, 256, 27, 27]               0\n",
            "         MaxPool2d-6          [-1, 256, 13, 13]               0\n",
            "            Conv2d-7          [-1, 384, 13, 13]         885,120\n",
            "              ReLU-8          [-1, 384, 13, 13]               0\n",
            "            Conv2d-9          [-1, 384, 13, 13]       1,327,488\n",
            "             ReLU-10          [-1, 384, 13, 13]               0\n",
            "           Conv2d-11          [-1, 256, 13, 13]         884,992\n",
            "             ReLU-12          [-1, 256, 13, 13]               0\n",
            "        MaxPool2d-13            [-1, 256, 6, 6]               0\n",
            "           Linear-14                 [-1, 4096]      37,752,832\n",
            "             ReLU-15                 [-1, 4096]               0\n",
            "          Dropout-16                 [-1, 4096]               0\n",
            "           Linear-17                 [-1, 4096]      16,781,312\n",
            "             ReLU-18                 [-1, 4096]               0\n",
            "          Dropout-19                 [-1, 4096]               0\n",
            "           Linear-20                   [-1, 10]          40,970\n",
            "================================================================\n",
            "Total params: 58,322,314\n",
            "Trainable params: 58,322,314\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.59\n",
            "Forward/backward pass size (MB): 11.04\n",
            "Params size (MB): 222.48\n",
            "Estimated Total Size (MB): 234.11\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training on cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1, train loss 181.6218, train acc 0.328, test loss 147.2314, test acc 0.456, time 98.0 sec\n",
            "epoch 2, train loss 143.1644, train acc 0.482, test loss 125.6369, test acc 0.548, time 99.2 sec\n",
            "epoch 3, train loss 125.4468, train acc 0.551, test loss 114.4990, test acc 0.594, time 99.0 sec\n",
            "epoch 4, train loss 111.6750, train acc 0.605, test loss 104.9471, test acc 0.628, time 98.4 sec\n",
            "epoch 5, train loss 100.7891, train acc 0.644, test loss 99.2992, test acc 0.645, time 99.0 sec\n",
            "epoch 6, train loss 92.0496, train acc 0.677, test loss 96.1359, test acc 0.657, time 98.8 sec\n",
            "epoch 7, train loss 84.0051, train acc 0.706, test loss 88.9427, test acc 0.695, time 97.9 sec\n",
            "epoch 8, train loss 77.7025, train acc 0.729, test loss 87.1120, test acc 0.706, time 99.0 sec\n",
            "epoch 9, train loss 72.4054, train acc 0.746, test loss 85.3255, test acc 0.710, time 98.8 sec\n",
            "epoch 10, train loss 66.8329, train acc 0.765, test loss 87.8164, test acc 0.704, time 98.0 sec\n",
            "epoch 11, train loss 62.9326, train acc 0.780, test loss 83.1579, test acc 0.724, time 97.8 sec\n",
            "epoch 12, train loss 57.7135, train acc 0.797, test loss 83.4133, test acc 0.723, time 98.8 sec\n",
            "epoch 13, train loss 53.4118, train acc 0.811, test loss 87.1946, test acc 0.720, time 99.4 sec\n",
            "epoch 14, train loss 50.3212, train acc 0.822, test loss 87.3642, test acc 0.727, time 98.0 sec\n",
            "epoch 15, train loss 48.5358, train acc 0.830, test loss 89.9714, test acc 0.707, time 99.1 sec\n",
            "epoch 16, train loss 45.9841, train acc 0.838, test loss 84.4436, test acc 0.724, time 99.2 sec\n",
            "epoch 17, train loss 43.4075, train acc 0.850, test loss 89.7248, test acc 0.717, time 99.8 sec\n",
            "epoch 18, train loss 41.6755, train acc 0.855, test loss 89.4191, test acc 0.720, time 97.9 sec\n",
            "epoch 19, train loss 40.0428, train acc 0.861, test loss 95.7196, test acc 0.708, time 99.1 sec\n",
            "epoch 20, train loss 38.3274, train acc 0.867, test loss 90.7256, test acc 0.717, time 98.8 sec\n"
          ]
        }
      ],
      "source": [
        "# Implementa sua rede neural aqui\n",
        "# Voce pode implementar como uma classe (pois fica mais organizado e é mais facilitado para um debug da rede)\n",
        "# Dica: utilize blocos de sequential para diminuir a complexidade da função de forward\n",
        "# rede baseada na AlexNet\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, input_channels, classes=10, **kwargs):\n",
        "        super(AlexNet, self).__init__(**kwargs)\n",
        "        # Defina aqui a arquitetura da rede\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(input_channels, 96, kernel_size=11, stride=4),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(96, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 384, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(4096, classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # defina aqui a função de forward\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        out = self.classifier(x)\n",
        "        return out\n",
        "\n",
        "# parâmetros: número de epochs, learning rate (ou taxa de aprendizado),\n",
        "# tamanho do batch, e lambda do weight decay\n",
        "num_epochs, lr, batch_size, wd_lambda = 20, 0.001, 100, 0.0001\n",
        "\n",
        "net = AlexNet(input_channels=3, classes=10)\n",
        "# Sending model to device\n",
        "net.to(device)\n",
        "\n",
        "print(summary(net,(3,227,227))) # visualizar o número de parâmetros da rede e output de cada camada\n",
        "\n",
        "# função de custo (ou loss)\n",
        "loss = nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "\n",
        "# carregamento do dado: cifar10\n",
        "train_iter, test_iter = load_data_cifar10(batch_size, resize=227, root=root)\n",
        "\n",
        "# definindo o otimizador\n",
        "trainer = optim.Adam(net.parameters(), lr=lr, weight_decay=wd_lambda)\n",
        "\n",
        "# treinamento e validação via Pytorch\n",
        "train_validate(net, train_iter, test_iter, batch_size, trainer, loss, num_epochs)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}